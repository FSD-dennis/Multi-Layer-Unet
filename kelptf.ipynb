{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T02:54:32.350365600Z",
     "start_time": "2024-02-19T02:54:32.311404400Z"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "rootPath = \"D:\\\\kelp\"\n",
    "train_folder = os.path.join(rootPath, \"train_satellite\")\n",
    "train_label = os.path.join(rootPath, \"train_kelp\")\n",
    "metadata = pd.read_csv(os.path.join(rootPath, \"metadata_fTq0l2T.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Layer, Conv2D, Concatenate, MaxPooling2D, Dropout, Conv2DTranspose, Activation, Reshape, ConvLSTM2D, Input, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_list = []\n",
    "train_mask_list = []\n",
    "predict_image_list = []\n",
    "\n",
    "for index, row in metadata.iterrows():\n",
    "    filename = row['filename']\n",
    "    if row[\"in_train\"] == True and row[\"type\"] == \"satellite\":\n",
    "        train_image_list.append(filename)\n",
    "    elif row[\"in_train\"] == False and row[\"type\"] == \"kelp\":\n",
    "        train_mask_list.append(filename)\n",
    "    else:\n",
    "        predict_image_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_list.sort()\n",
    "train_mask_list.sort()\n",
    "predict_image_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guoyy\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\rasterio\\__init__.py:220: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "with rasterio.open(os.path.join(train_folder,train_image_list[1])) as src:\n",
    "    data = src.read()  # Reads the raster data as a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleconv2d(input_size, filter):\n",
    "    temp = Conv2D(filter, 3, padding= \"same\", activation=\"relu\", kernel_initializer= \"he_normal\")(input_size)\n",
    "    output = Conv2D(filter, 3, padding= \"same\", activation=\"relu\", kernel_initializer= \"he_normal\")(temp)\n",
    "    return output\n",
    "\n",
    "def down2d(input_size, filter):\n",
    "    mapping = doubleconv2d(input_size, filter)\n",
    "    temp2 = MaxPooling2D(2)(mapping)\n",
    "    output = Dropout(0.1)(temp2)\n",
    "    return mapping, output\n",
    "\n",
    "def un2d(input_size, mapping, filter, shape):\n",
    "    TransConv = Conv2DTranspose(filter, 3, 2, padding=\"same\")(input_size)\n",
    "    BN = BatchNormalization(axis=3)(TransConv)\n",
    "    ACT = Activation('relu')(BN)\n",
    "    \n",
    "    mapping = tf.image.resize_with_crop_or_pad(mapping, np.int32(shape),np.int32(shape))\n",
    "    ACT = tf.image.resize_with_crop_or_pad(ACT, np.int32(shape),np.int32(shape))\n",
    "\n",
    "    MAP = Reshape(target_shape=(1, np.int32(shape), np.int32(shape), filter))(mapping)\n",
    "    UP = Reshape(target_shape=(1, np.int32(shape), np.int32(shape), filter))(ACT)\n",
    "    UP = Concatenate(axis = 1)([MAP,UP])\n",
    "    UP = ConvLSTM2D(filters = filter, \n",
    "                            kernel_size=(3, 3), \n",
    "                            padding='same', \n",
    "                            return_sequences = False, \n",
    "                            go_backwards = True, \n",
    "                            kernel_initializer = 'he_normal')(UP)\n",
    "    UP = Dropout(0.1)(UP)\n",
    "    UP = doubleconv2d(UP, filter)\n",
    "    return UP\n",
    "\n",
    "def make_unet(input_size = (350, 350, 4)):\n",
    "    inputs = Input(input_size)\n",
    "    map1, down1 = down2d(inputs, 64)\n",
    "    map2, down2 = down2d(down1, 128)\n",
    "    map3, down3 = down2d(down2, 256)\n",
    "    \n",
    "    bottleneck1 = doubleconv2d(down3, 512)\n",
    "    bottleneck2 = doubleconv2d(bottleneck1, 512)\n",
    "    merge_dense = Concatenate(axis=3)([bottleneck1,bottleneck2])\n",
    "    bottleneck3 = doubleconv2d(merge_dense, 512)\n",
    "\n",
    "    up1 = un2d(bottleneck3, map3, 256, input_size[0]/4)\n",
    "    up2 = un2d(up1, map2, 128, input_size[0]/2)\n",
    "    up3 = un2d(up2, map1, 64, input_size[0])\n",
    "\n",
    "    outputs = Conv2D(1, 1, padding=\"same\", activation = \"sigmoid\")(up3)\n",
    "    unet_model = Model(inputs, outputs, name=\"U-Net\")\n",
    "    \n",
    "    return unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_net = make_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"U-Net\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 350, 350, 4  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 350, 350, 64  2368        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 350, 350, 64  36928       ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 175, 175, 64  0           ['conv2d_1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 175, 175, 64  0           ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 175, 175, 12  73856       ['dropout[0][0]']                \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 175, 175, 12  147584      ['conv2d_2[0][0]']               \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 87, 87, 128)  0          ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 87, 87, 128)  0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 87, 87, 256)  295168      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 87, 87, 256)  590080      ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 43, 43, 256)  0          ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 43, 43, 256)  0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 43, 43, 512)  1180160     ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 43, 43, 512)  2359808     ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 43, 43, 512)  2359808     ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 43, 43, 512)  2359808     ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 43, 43, 1024  0           ['conv2d_7[0][0]',               \n",
      "                                )                                 'conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 43, 43, 512)  4719104     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 43, 43, 512)  2359808     ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 86, 86, 256)  1179904    ['conv2d_11[0][0]']              \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 86, 86, 256)  1024       ['conv2d_transpose[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 86, 86, 256)  0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " tf.image.resize_with_crop_or_p  (None, 87, 87, 256)  0          ['conv2d_5[0][0]']               \n",
      " ad (TFOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.image.resize_with_crop_or_p  (None, 87, 87, 256)  0          ['activation[0][0]']             \n",
      " ad_1 (TFOpLambda)                                                                                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 87, 87, 2  0           ['tf.image.resize_with_crop_or_pa\n",
      "                                56)                              d[0][0]']                        \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 87, 87, 2  0           ['tf.image.resize_with_crop_or_pa\n",
      "                                56)                              d_1[0][0]']                      \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 2, 87, 87, 2  0           ['reshape[0][0]',                \n",
      "                                56)                               'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv_lstm2d (ConvLSTM2D)       (None, 87, 87, 256)  4719616     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 87, 87, 256)  0           ['conv_lstm2d[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 87, 87, 256)  590080      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 87, 87, 256)  590080      ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 174, 174, 12  295040     ['conv2d_13[0][0]']              \n",
      " spose)                         8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 174, 174, 12  512        ['conv2d_transpose_1[0][0]']     \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 174, 174, 12  0           ['batch_normalization_1[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " tf.image.resize_with_crop_or_p  (None, 175, 175, 12  0          ['conv2d_3[0][0]']               \n",
      " ad_2 (TFOpLambda)              8)                                                                \n",
      "                                                                                                  \n",
      " tf.image.resize_with_crop_or_p  (None, 175, 175, 12  0          ['activation_1[0][0]']           \n",
      " ad_3 (TFOpLambda)              8)                                                                \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 1, 175, 175,  0           ['tf.image.resize_with_crop_or_pa\n",
      "                                 128)                            d_2[0][0]']                      \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 1, 175, 175,  0           ['tf.image.resize_with_crop_or_pa\n",
      "                                 128)                            d_3[0][0]']                      \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 2, 175, 175,  0           ['reshape_2[0][0]',              \n",
      "                                 128)                             'reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv_lstm2d_1 (ConvLSTM2D)     (None, 175, 175, 12  1180160     ['concatenate_2[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 175, 175, 12  0           ['conv_lstm2d_1[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 175, 175, 12  147584      ['dropout_4[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 175, 175, 12  147584      ['conv2d_14[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 350, 350, 64  73792      ['conv2d_15[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 350, 350, 64  256        ['conv2d_transpose_2[0][0]']     \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 350, 350, 64  0           ['batch_normalization_2[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.image.resize_with_crop_or_p  (None, 350, 350, 64  0          ['conv2d_1[0][0]']               \n",
      " ad_4 (TFOpLambda)              )                                                                 \n",
      "                                                                                                  \n",
      " tf.image.resize_with_crop_or_p  (None, 350, 350, 64  0          ['activation_2[0][0]']           \n",
      " ad_5 (TFOpLambda)              )                                                                 \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 1, 350, 350,  0           ['tf.image.resize_with_crop_or_pa\n",
      "                                 64)                             d_4[0][0]']                      \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 1, 350, 350,  0           ['tf.image.resize_with_crop_or_pa\n",
      "                                 64)                             d_5[0][0]']                      \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 2, 350, 350,  0           ['reshape_4[0][0]',              \n",
      "                                 64)                              'reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv_lstm2d_2 (ConvLSTM2D)     (None, 350, 350, 64  295168      ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 350, 350, 64  0           ['conv_lstm2d_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 350, 350, 64  36928       ['dropout_5[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 350, 350, 64  36928       ['conv2d_16[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 350, 350, 1)  65          ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,779,201\n",
      "Trainable params: 25,778,305\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "u_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Generate the plot\n",
    "plot_model(u_net, to_file='u_net_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guoyy\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\rasterio\\__init__.py:220: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with rasterio.open(os.path.join(train_folder,train_image_list[1])) as src:\n",
    "    data = src.read()  # Reads the raster data as a numpy array\n",
    "\n",
    "type(data[[0,1,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5635"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5635"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ince_dataset(validation = False):\n",
    "    x, y = [], []\n",
    "    if validation:\n",
    "        for i, (image, mask) in enumerate(zip(train_image_list[1000:1201], train_mask_list[1000:1201])):\n",
    "            print(\"\\r\"+str(i+1)+\"/\"+str(1200),end=\"\")\n",
    "            with rasterio.open(os.path.join(train_folder, train_image_list[i])) as ima:\n",
    "                image = ima.read()\n",
    "            image = image[[0,1,5,6]]\n",
    "            \n",
    "            with rasterio.open(os.path.join(train_label, train_mask_list[i])) as ma:\n",
    "                mask = ma.read()\n",
    "                \n",
    "            image = (image-np.min(image))/(np.max(image) - np.min(image) + 1e-10)\n",
    "            mask = (mask-np.min(mask))/(np.max(mask) - np.min(mask) + 1e-10)\n",
    "\n",
    "            x.append(image)\n",
    "            y.append(mask)\n",
    "    else:\n",
    "        for i, (image, mask) in enumerate(zip(train_image_list[:1000], train_mask_list[:1000])):\n",
    "            print(\"\\r\"+str(i+1)+\"/\"+str(1000),end=\"\")\n",
    "            with rasterio.open(os.path.join(train_folder, train_image_list[i])) as ima:\n",
    "                image = ima.read()\n",
    "            image = image[[0,1,5,6]]\n",
    "            \n",
    "            with rasterio.open(os.path.join(train_label, train_mask_list[i])) as ma:\n",
    "                mask = ma.read()\n",
    "                \n",
    "            image = (image-np.min(image))/(np.max(image) - np.min(image) + 1e-10)\n",
    "            mask = (mask-np.min(mask))/(np.max(mask) - np.min(mask) + 1e-10)\n",
    "\n",
    "            x.append(image)\n",
    "            y.append(mask)\n",
    "    return np.array(x), np.array(y)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/1000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guoyy\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\rasterio\\__init__.py:220: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201/12000"
     ]
    }
   ],
   "source": [
    "x,y = make_ince_dataset()\n",
    "v_x,v_y = make_ince_dataset(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 350, 350)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_net.compile(\n",
    "    optimizer=\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transfer our image data into two types of tensor and the label into another tensor**\n",
    "1. (350, 350, 4) for infrared, near infrared, cloud mask, elevation. we call that \"ince\"\n",
    "2. (350, 350, 3) for RGB, we call that \"rgb\"\n",
    "3. (350, 350, 1) for label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create DataLoader for each type of dataset**\n",
    "\n",
    "**I need to consider the \"ince\" and the \"rgb\" data**\n",
    "\n",
    "| Dataloader Type    | Dataloader Name |\n",
    "|-------------------|-----------------|\n",
    "| Train Dataloader  | 1. train_ince_dl |\n",
    "|                   | 2. train_rgb_dl  |\n",
    "| Val Dataloader    | 3. val_ince_dl   |\n",
    "|                   | 4. val_rgb_dl    |\n",
    "| Test Dataloader   | 5. test_ince_dl  |\n",
    "|                   | 6. test_rgb_dl   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T02:54:35.200718600Z",
     "start_time": "2024-02-19T02:54:35.179464800Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ince_dl_temp       =     TrainDataset(train_dir = train_dir, label_dir = label_dir, type = \"train\", val_size = 0.2, sep = \"ince\")\n",
    "train_rgb_dl_temp        =     TrainDataset(train_dir = train_dir, label_dir = label_dir, type = \"train\", val_size = 0.2, sep = \"rgb\")\n",
    "val_ince_dl_temp         =     TrainDataset(train_dir = train_dir, label_dir = label_dir, type = \"val\", val_size = 0.2, sep = \"ince\")\n",
    "val_rgb_dl_temp          =     TrainDataset(train_dir = train_dir, label_dir = label_dir, type = \"val\", val_size = 0.2, sep = \"rgb\")\n",
    "test_ince_rgb_dl_temp    =     TestDataset(test_dir = test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T02:54:35.200718600Z",
     "start_time": "2024-02-19T02:54:35.179464800Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ince_dl       =     DataLoader(train_ince_dl_temp, batch_size = 8)\n",
    "train_rgb_dl        =     DataLoader(train_rgb_dl_temp, batch_size = 8)\n",
    "val_ince_dl         =     DataLoader(val_ince_dl_temp, batch_size = 8)\n",
    "val_rgb_dl          =     DataLoader(val_rgb_dl_temp, batch_size = 8)\n",
    "test_ince_rgb_dl    =     DataLoader(test_ince_rgb_dl_temp, batch_size = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create Unet and training function**\n",
    "\n",
    "*allow me to make an estimation first and inspection usnig torchsummary first*\n",
    "\n",
    "- initiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "incemodel = UShapeNet(4).to(device=device)\n",
    "# summary(incemodel, (4,350,350));\n",
    "rgbmodel = UShapeNet(3).to(device=device)\n",
    "# summary(rgbmodel, (3,350,350));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**define the training process!  together with our DataLoader**\n",
    "- training ince model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:08<00:00,  1.73s/it] ?it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.77it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.45it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]00:11<00:44, 11.07s/it]\n",
      "out of epochs:  20%|██        | 1/5 [00:11<00:45, 11.30s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3836\\3565984194.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCombinedLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_dice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_ce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mincemodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtrain_accuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrain_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mval_accuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"out of epochs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtrain_epoch_accuracy_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\guoyy\\miniconda3\\envs\\torchcv\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1189\u001b[0m                         \u001b[0mlast_print_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_print_n\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m                         \u001b[0mlast_print_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_print_t\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\guoyy\\miniconda3\\envs\\torchcv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    639\u001b[0m                     warn_msg += (\"For multiprocessing data-loading, this could be caused by not properly configuring the \"\n\u001b[0;32m    640\u001b[0m                                  \u001b[1;34m\"IterableDataset replica at each worker. Please see \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m                                  \"https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\")\n\u001b[0;32m    642\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwarn_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\guoyy\\miniconda3\\envs\\torchcv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\guoyy\\miniconda3\\envs\\torchcv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__getitems__\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\guoyy\\miniconda3\\envs\\torchcv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__getitems__\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3836\\2654822397.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtiff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTiffFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtrain_temp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mtrain_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_temp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtiff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTiffFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlabel_temp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mlabel_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_temp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\guoyy\\miniconda3\\envs\\torchcv\\lib\\site-packages\\tifffile\\tifffile.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, series, level, squeeze, out, maxworkers)\u001b[0m\n\u001b[0;32m   4261\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4262\u001b[0m             \u001b[0mpage0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4263\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpage0\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4264\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'page is None'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4265\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxworkers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4266\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4267\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack_pages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxworkers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\guoyy\\miniconda3\\envs\\torchcv\\lib\\site-packages\\tifffile\\tifffile.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, out, squeeze, lock, maxworkers)\u001b[0m\n\u001b[0;32m   8919\u001b[0m                     ]\n\u001b[0;32m   8920\u001b[0m                 \u001b[1;31m# except IndexError:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8921\u001b[0m                 \u001b[1;31m#     pass  # corrupted file, e.g., with too many strips\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8923\u001b[1;33m             for _ in self.segments(\n\u001b[0m\u001b[0;32m   8924\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8925\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlock\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8926\u001b[0m                 \u001b[0mmaxworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\guoyy\\miniconda3\\envs\\torchcv\\lib\\site-packages\\tifffile\\tifffile.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, lock, maxworkers, func, sort, _fullsize)\u001b[0m\n\u001b[0;32m   8720\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlock\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8721\u001b[0m                 \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8722\u001b[0m                 \u001b[0mflat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8723\u001b[0m             ):\n\u001b[1;32m-> 8724\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8725\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8726\u001b[0m             \u001b[1;31m# reduce memory overhead by processing chunks of up to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8727\u001b[0m             \u001b[1;31m# ~256 MB of segments because ThreadPoolExecutor.map is not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\guoyy\\miniconda3\\envs\\torchcv\\lib\\site-packages\\tifffile\\tifffile.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(args, decodeargs, decode)\u001b[0m\n\u001b[0;32m   8711\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecodeargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecodeargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeyframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8712\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mdecodeargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\guoyy\\miniconda3\\envs\\torchcv\\lib\\site-packages\\tifffile\\tifffile.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(data, index, jpegtables, jpegheader, _fullsize)\u001b[0m\n\u001b[0;32m   8637\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimagecodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbitorder_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8638\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdecompress\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8639\u001b[0m                 \u001b[1;31m# TODO: calculate correct size for packed integers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8640\u001b[0m                 \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8641\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8642\u001b[0m             \u001b[0mdata_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8643\u001b[0m             \u001b[1;31m# del data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8644\u001b[0m             \u001b[0mdata_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegmentindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = CombinedLoss(weight_dice=0.5, weight_ce=0.5)\n",
    "optimizer = torch.optim.Adam(incemodel.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "for epoch in trange(num_epochs, desc=\"out of epochs\", leave=True):\n",
    "    train_epoch_accuracy_list = []\n",
    "    train_epoch_batch_loss = []\n",
    "    val_epoch_accuracy_list = []\n",
    "    for index, batch in enumerate(tqdm(iter(train_ince_dl))):\n",
    "        x, y = batch\n",
    "        batch_loss = train_batch(x, y, incemodel, loss_fn, optimizer)\n",
    "        train_epoch_batch_loss.append(batch_loss)\n",
    "        \n",
    "    for index, batch in enumerate(tqdm(iter(train_ince_dl))):\n",
    "        x, y = batch\n",
    "        temp = accuracy(x, y, incemodel)\n",
    "        train_epoch_accuracy_list.append(temp)\n",
    "    \n",
    "    for index, batch in enumerate(tqdm(iter(val_ince_dl))):\n",
    "        x, y = batch\n",
    "        temp = accuracy(x, y, incemodel)\n",
    "        val_epoch_accuracy_list.append(temp)\n",
    "        \n",
    "    train_accuracies.append(np.mean(train_epoch_accuracy_list))\n",
    "    train_losses.append(np.mean(train_epoch_batch_loss))\n",
    "    val_accuracies.append(np.mean(val_epoch_accuracy_list))\n",
    "    \n",
    "print(f\"train_accuracies is {train_accuracies}\")\n",
    "print(f\"train_losses is {train_losses}\")\n",
    "print(f\"val_accuracies is {val_accuracies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save the ince model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(incemodel.state_dict(), os.path.join(modelPath,'ince_model_params.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train rgb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.49it/s] ?it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.58it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.21it/s]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.58it/s]:23,  5.78s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.63it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.41it/s]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.58it/s]:16,  5.63s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.74it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.53it/s]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.62it/s]:11,  5.55s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.73it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.48it/s]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.53it/s]:05,  5.48s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.57it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.15it/s]\n",
      "out of epochs: 100%|██████████| 5/5 [00:27<00:00,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracies is [0.59146464, 0.992591, 0.992591, 0.992591, 0.992591]\n",
      "train_losses is [0.525196635723114, 1.7771829605102538, 0.28945243954658506, 0.28411559462547303, 0.2776166796684265]\n",
      "val_accuracies is [0.54165256, 0.99517804, 0.99517804, 0.99517804, 0.99517804]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = CombinedLoss(weight_dice=0.5, weight_ce=0.5)\n",
    "optimizer = torch.optim.Adam(rgbmodel.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "for epoch in trange(num_epochs, desc=\"out of epochs\", leave=True):\n",
    "    train_epoch_accuracy_list = []\n",
    "    train_epoch_batch_loss = []\n",
    "    val_epoch_accuracy_list = []\n",
    "    for index, batch in enumerate(tqdm(iter(train_rgb_dl))):\n",
    "        x, y = batch\n",
    "        batch_loss = train_batch(x, y, rgbmodel, loss_fn, optimizer)\n",
    "        train_epoch_batch_loss.append(batch_loss)\n",
    "        \n",
    "    for index, batch in enumerate(tqdm(iter(train_rgb_dl))):\n",
    "        x, y = batch\n",
    "        temp = accuracy(x, y, rgbmodel)\n",
    "        train_epoch_accuracy_list.append(temp)\n",
    "    \n",
    "    for index, batch in enumerate(tqdm(iter(val_rgb_dl))):\n",
    "        x, y = batch\n",
    "        temp = accuracy(x, y, rgbmodel)\n",
    "        val_epoch_accuracy_list.append(temp)\n",
    "        \n",
    "    train_accuracies.append(np.mean(train_epoch_accuracy_list))\n",
    "    train_losses.append(np.mean(train_epoch_batch_loss))\n",
    "    val_accuracies.append(np.mean(val_epoch_accuracy_list))\n",
    "    \n",
    "print(f\"train_accuracies is {train_accuracies}\")\n",
    "print(f\"train_losses is {train_losses}\")\n",
    "print(f\"val_accuracies is {val_accuracies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save the rgb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rgbmodel.state_dict(), os.path.join(modelPath,'rgb_model_params.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**vaccuracy and loss**\n",
    "- it is super good after one epic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.59146464, 0.992591, 0.992591, 0.992591, 0.992591]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.54165256, 0.99517804, 0.99517804, 0.99517804, 0.99517804]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model combined the rgb and ince together!\n",
    "\n",
    "    how to combine the (350, 350, 2) and (350, 350, 2) together??\n",
    "\n",
    "    It is in the [test.ipynb](https://github.com/y1u2a3n4g5/Multi-Layer-Unet/blob/main/test.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visulization of the Unet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "# rgbmodel.eval()\n",
    "# for i in rgbmodel.parameters():\n",
    "#     i.requires_grad = True\n",
    "# dummy_input = torch.randn(8, 3, 350, 350).to(device=device)\n",
    "# y = rgbmodel(dummy_input)\n",
    "# graph = make_dot(y, params=dict(rgbmodel.named_parameters()))\n",
    "# graph.render('rgbmodel', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'incemodel.png'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "incemodel.eval()\n",
    "for i in incemodel.parameters():\n",
    "    i.requires_grad = True\n",
    "dummy_input = torch.randn(8, 4, 350, 350).to(device=device)\n",
    "y = incemodel(dummy_input)\n",
    "graph = make_dot(y, params=dict(incemodel.named_parameters()))\n",
    "graph.render('incemodel', format='png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
